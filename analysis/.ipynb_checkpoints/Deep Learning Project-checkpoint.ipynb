{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning Program\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#Tensor Flow imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/home/narcis/Escriptori/Màster/Research and Innovation/Introduction to Deep Learning/Project/dataset-ordenat\"\n",
    "\n",
    "CATEGORIES = [\"aa\",\"aà\",\"aá\",\"aä\",\"aâ\",\"aā\",\n",
    "             \"ee\",\"eè\",\"eé\",\"eë\",\"eê\",\"eē\",\n",
    "             \"ii\",\"iì\",\"ií\",\"iï\",\"iî\",\"iī\",\n",
    "             \"oo\",\"oò\",\"oó\",\"oö\",\"oô\",\"oō\",\n",
    "             \"uu\",\"uù\",\"uú\",\"uü\",\"uû\",\"uū\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training data\n",
    "IMG_SIZE=50\n",
    "training_data=[]\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category) #Path to dir\n",
    "        class_num=CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            #try:\n",
    "                img_array= cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE) #Grayscale\n",
    "                new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) #Resize\n",
    "                training_data.append([new_array,class_num])\n",
    "            #except Exception as e: \"\"\"Crec que això és per si hi ha altres carpetes al directori\"\"\"\n",
    "            #    pass\n",
    "        \n",
    "create_training_data()\n",
    "random.shuffle(training_data) #Shuffle Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "[array([[255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       ...,\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255],\n",
      "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8), 7]\n"
     ]
    }
   ],
   "source": [
    "#Test per veure que fa\n",
    "print(len(training_data))\n",
    "print(training_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oō\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Separe Images and freatures\n",
    "x = []\n",
    "y = []\n",
    "for features, label in training_data:\n",
    "    x.append(features)\n",
    "    y.append(label)\n",
    "x = np.array(x).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "#Normalize data \"\"\"S'haura de provar si funciona o que\"\"\"\n",
    "x = x/255.0\n",
    "\n",
    "# X constains the image information and y -- Test\n",
    "plt.imshow(x[61],cmap=\"gray\")\n",
    "print(CATEGORIES[int(y[61])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# We need to split the data between the test and the train\n",
    "#val_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "DATASET_SIZE = 2000\n",
    "\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "test_size = int(0.3* DATASET_SIZE)\n",
    "\n",
    "print(train_size)\n",
    "print(test_size)\n",
    "\n",
    "#train_dataset = full_dataset.take(train_size)\n",
    "#test_dataset = full_dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu)) #128 neurons\n",
    "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu)) #relu activation function\n",
    "model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu)) #relu activation function\n",
    "model.add(tf.keras.layers.Dense(30,activation=tf.nn.softmax))\n",
    "\n",
    "#Training, adam optimizer\n",
    "model.compile(optimizer=\"adam\",\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4686 - accuracy: 0.0410\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3761 - accuracy: 0.0485\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1489 - accuracy: 0.0735\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9310 - accuracy: 0.0835\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.8394 - accuracy: 0.0960\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7196 - accuracy: 0.1135\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6640 - accuracy: 0.1120\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6528 - accuracy: 0.1235\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5338 - accuracy: 0.1350\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4108 - accuracy: 0.1570\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4117 - accuracy: 0.1620\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2554 - accuracy: 0.1765\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2052 - accuracy: 0.1865\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2162 - accuracy: 0.1925\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1569 - accuracy: 0.1885\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0764 - accuracy: 0.2060\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0243 - accuracy: 0.2250\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1286 - accuracy: 0.1975\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9968 - accuracy: 0.2280\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0010 - accuracy: 0.2300\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9948 - accuracy: 0.2320\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0174 - accuracy: 0.2355\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0449 - accuracy: 0.2255\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8980 - accuracy: 0.2565\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9348 - accuracy: 0.2450\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9656 - accuracy: 0.2405\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8474 - accuracy: 0.2690\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8627 - accuracy: 0.2565\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8142 - accuracy: 0.2890\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8679 - accuracy: 0.2695\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8313 - accuracy: 0.2755\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9711 - accuracy: 0.2450\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8058 - accuracy: 0.2925\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7891 - accuracy: 0.2845\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7858 - accuracy: 0.2935\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7819 - accuracy: 0.2935\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7958 - accuracy: 0.2905\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8297 - accuracy: 0.2900\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6859 - accuracy: 0.3295\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7021 - accuracy: 0.3150\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6691 - accuracy: 0.3410\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6851 - accuracy: 0.3170\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7154 - accuracy: 0.3270\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6234 - accuracy: 0.3500\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5958 - accuracy: 0.3595\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7275 - accuracy: 0.3240\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6506 - accuracy: 0.3605\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6497 - accuracy: 0.3540\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5911 - accuracy: 0.3675\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5709 - accuracy: 0.3760\n",
      "WARNING:tensorflow:From /home/narcis/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/narcis/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: test1.model/assets\n"
     ]
    }
   ],
   "source": [
    "#Model Fit\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "model.fit(x, y, epochs=50)\n",
    "\n",
    "#Save the model\n",
    "model.save(\"test1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare file to predict // Test\n",
    "\n",
    "def prepare(filepath):\n",
    "    IMG_SIZE = 50\n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "    return new_array.reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "model = tf.keras.models.load_model(\"test1.model\")\n",
    "\n",
    "#prediction = model.predict([prepare(\"\")])\n",
    "#print(prediction)\n",
    "#print(len(prediction))\n",
    "#prediction = model.predict([prepare(\"a.png\")])\n",
    "#print(CATEGORIES[int(prediction[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
